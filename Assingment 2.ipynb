{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Assignment 2-A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"/Users/sofiagarfias/Documents/Machine Learning /ML_Assignment_ii_A_201920 (1)/kwb.csv\"\n",
    "df=pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['recs'] == 'Buurt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['gwb_code_10','gwb_code', 'gwb_code_8', 'regio', 'gm_naam', 'recs', 'ind_wbi'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.replace('.', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['g_ele','g_gas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target=df[[\"g_ele\",\"g_gas\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.dtypes\n",
    "df_target=df_target.astype(\"int64\")\n",
    "df_target[\"g_ele\"]=pd.qcut(df_target[\"g_ele\"],q=4,labels=False)\n",
    "df_target[\"g_gas\"]=pd.qcut(df_target[\"g_gas\"],q=4,labels=False)\n",
    "df=df.drop(['g_gas','g_ele'],axis=1)\n",
    "df_target.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We still have missing values in our training dataset. However these will be imputed in the following question.\n",
    "\n",
    "\n",
    "* The observations with missing values for the labels have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Feature Imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    \n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    \n",
    "    return roc_auc_score(y_test, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from scikit documentation\n",
    "def binarize_getroc(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    n_classes = y_test.shape[1]\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "gas_fpr = {}\n",
    "gas_tpr = {}\n",
    "gas_roc_auc = {}\n",
    "\n",
    "ele_fpr = {}\n",
    "ele_tpr = {}\n",
    "ele_roc_auc = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"g_gas\" Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df, df_target['g_gas']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_gas\" Unscaled Linear SVC model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_non_scaled = make_pipeline(SimpleImputer(strategy='mean'), LinearSVC(C=1, random_state = 42))\n",
    "linear_svc_non_scaled.fit(X_train, y_train)\n",
    "linear_svc_non_scaled.score(X_test, y_test)\n",
    "y_pred = linear_svc_non_scaled.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, y_pred))) \n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_gas\" Scaled Linear SVC model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_scaled = make_pipeline(SimpleImputer(strategy='mean'), MinMaxScaler(), LinearSVC(C=1, random_state = 42))\n",
    "linear_svc_scaled.fit(X_train, y_train)\n",
    "linear_svc_scaled.score(X_test, y_test)\n",
    "y_pred = linear_svc_scaled.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, y_pred))) \n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)\n",
    "\n",
    "\n",
    "model_name = 'linear svc scaled'\n",
    "gas_fpr[model_name], gas_tpr[model_name], gas_roc_auc[model_name] = binarize_getroc(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The confusion matrix is summarised using *precision* and *recall*.\n",
    "\n",
    "\n",
    "* Our value for precision which measures how many of the samples predicted as positive are actually positive is:\n",
    "    * **Unscaled SVC**: 0.72\n",
    "    * **Scaled SVC**: 0.68\n",
    "    * We can infer from this that even though the Unscaled SVC is higher they are quite similar and it is important to note that the Unscaled SVC did not converge. Thus, the valididty of this metric might be negatively affected and should not be compared to the Scaled score. \n",
    "     \n",
    "     \n",
    "* Our value for recall measures just how many of our samples were labeled correctly. A higher recall value shows that the model does not produce many false positives.\n",
    "    * **Unscaled SVC**: 0.37\n",
    "    * **Scaled SVC**: 0.69\n",
    "    * From this score we can infer that scaling the model produces far better results. This is to be expected as the SVC is model is quite sensitive to scaling. \n",
    "        \n",
    "        \n",
    "* The f-score does a very good job of summarising both recall and precision. \n",
    "    * **Uscaled SVC**:0.32\n",
    "    * **Scaled SVC**: 0.68\n",
    "    * This clearly shows that in total the scaled model is superior.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  \"g_gas\"  Unscaled Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_non_scaled = make_pipeline(SimpleImputer(strategy='mean'), LogisticRegression(solver=\"liblinear\", multi_class = 'ovr', random_state = 42))\n",
    "param_grid = {'logisticregression__C': [.001, 1, 100]}\n",
    "grid = GridSearchCV(log_reg_non_scaled, param_grid, cv=10, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)\n",
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(grid.best_params_))\n",
    "print('Training accuracy:{}'.format(grid.score(X_train, y_train)))\n",
    "print('Test accuracy:{}'.format(grid.score(X_test, y_test)))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \"g_gas\" Scaled Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_scaled = make_pipeline(SimpleImputer(strategy='mean'), MinMaxScaler(), LogisticRegression(solver=\"liblinear\", multi_class = 'ovr', random_state = 42))\n",
    "param_grid = {'logisticregression__C': [.001, 1, 100]}\n",
    "grid = GridSearchCV(log_reg_scaled, param_grid, cv=10, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)\n",
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(grid.best_params_))\n",
    "print('Training accuracy:{}'.format(grid.score(X_train, y_train)))\n",
    "print('Test accuracy:{}'.format(grid.score(X_test, y_test)))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)\n",
    "\n",
    "model_name = 'logistic regression scaled'\n",
    "gas_fpr[model_name], gas_tpr[model_name], gas_roc_auc[model_name] = binarize_getroc(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The confusion matrix is summarised using *precision* and *recall*.\n",
    "\n",
    "* Our value for precision which measures how many of the samples predicted as positive are actually positive is:\n",
    "    * **Unscaled LR**: 0.71\n",
    "    * **Scaled LR**: 0.70\n",
    "    * We can infer from this Logistic Regression is not very sensitive to scaling which is why the precision metric has the same score. \n",
    "     \n",
    "     \n",
    "* Our value for recall measures just how many of our samples were labeled correctly. A higher recall value shows that the model does not produce many false positives.\n",
    "    * **Unscaled LR**: 0.72\n",
    "    * **Scaled LR**: 0.71\n",
    "    * We can infer from this Logistic Regression is not very sensitive to scaling which is why the precision metric has the same score. \n",
    "\n",
    "        \n",
    "* The f-score does a very good job of summarising both recall and precision. \n",
    "    * **Uscaled LR**:0.71\n",
    "    * **Scaled LR**: 0.70\n",
    "    * We can infer from this Logistic Regression is not very sensitive to scaling which is why the precision metric has the same score. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"g_ele\" Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df, df_target['g_ele']\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)\n",
    "## adding stratify=y increases the score of the non-scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_ele\" Unscaled Linear SVC model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_non_scaled = make_pipeline(SimpleImputer(strategy='mean'), LinearSVC(C=1, random_state = 42))\n",
    "linear_svc_non_scaled.fit(X_train, y_train)\n",
    "linear_svc_non_scaled.score(X_test, y_test)\n",
    "y_pred = linear_svc_non_scaled.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, y_pred))) \n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_ele\" Scaled Linear SVC model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_scaled = make_pipeline(SimpleImputer(strategy='mean'), MinMaxScaler(), LinearSVC(C=1, random_state = 42))\n",
    "linear_svc_scaled.fit(X_train, y_train)\n",
    "linear_svc_scaled.score(X_test, y_test)\n",
    "y_pred = linear_svc_scaled.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, y_pred))) \n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)\n",
    "\n",
    "model_name = 'linear svc scaled'\n",
    "ele_fpr[model_name], ele_tpr[model_name], ele_roc_auc[model_name] = binarize_getroc(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The confusion matrix is summarised using *precision* and *recall*.\n",
    "\n",
    "\n",
    "* Our value for precision which measures how many of the samples predicted as positive are actually positive is:\n",
    "    * **Unscaled SVC**: 0.48\n",
    "    * **Scaled SVC**: 0.69\n",
    "    * We can infer from this that even though the Unscaled SVC is higher they are quite similar and it is important to note that the Unscaled SVC did not converge. Thus, the valididty of this metric might be negatively affected and should not be compared to the Scaled score. \n",
    "     \n",
    "     \n",
    "* Our value for recall measures just how many of our samples were labeled correctly. A higher recall value shows that the model does not produce many false positives.\n",
    "    * **Unscaled SVC**: 0.43\n",
    "    * **Scaled SVC**: 0.69\n",
    "    * From this score we can infer that scaling the model produces far better results. This is to be expected as the SVC is model is quite sensitive to scaling. \n",
    "      \n",
    "      \n",
    "* The f-score does a very good job of summarising both recall and precision. \n",
    "    * **Uscaled SVC**:0.37\n",
    "    * **Scaled SVC**: 0.68\n",
    "    * This clearly shows that in total the scaled model is superior.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_ele\" Unscaled Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_non_scaled = make_pipeline(SimpleImputer(strategy='mean'), LogisticRegression(solver=\"liblinear\", multi_class = 'ovr', random_state = 42))\n",
    "param_grid  = {'C': [.001, 1, 100]}\n",
    "logreg_cv = GridSearchCV(log_reg_non_scaled, param_grid, cv=5,n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)\n",
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(grid.best_params_))\n",
    "print('Training accuracy:{}'.format(grid.score(X_train, y_train)))\n",
    "print('Test accuracy:{}'.format(grid.score(X_test, y_test)))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_ele\" Scaled Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_scaled = make_pipeline(SimpleImputer(strategy='mean'), MinMaxScaler(), LogisticRegression(solver=\"liblinear\", multi_class = 'ovr', random_state = 42))\n",
    "param_grid = {'logisticregression__C': [.001, 1, 100]}\n",
    "grid = GridSearchCV(log_reg_scaled, param_grid, cv=10, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.score(X_test, y_test)\n",
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(grid.best_params_))\n",
    "print(\"Best score is {}\".format(grid.best_score_))\n",
    "print('Training accuracy:{}'.format(grid.score(X_train, y_train)))\n",
    "print('Test accuracy:{}'.format(grid.score(X_test, y_test)))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)\n",
    "\n",
    "model_name = 'logistic regression scaled'\n",
    "ele_fpr[model_name], ele_tpr[model_name], ele_roc_auc[model_name] = binarize_getroc(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The confusion matrix is summarised using *precision* and *recall*.\n",
    "\n",
    "\n",
    "* Our value for precision which measures how many of the samples predicted as positive are actually positive is:\n",
    "    * **Unscaled LR**: 0.70\n",
    "    * **Scaled LR**: 0.70\n",
    "    * We can infer from this Logistic Regression is not very sensitive to scaling which is why the precision metric has the same score. \n",
    "    \n",
    "    \n",
    "* Our value for recall measures just how many of our samples were labeled correctly. A higher recall value shows that the model does not produce many false positives.\n",
    "    * **Unscaled LR**: 0.70\n",
    "    * **Scaled LR**: 0.70\n",
    "    * We can infer from this Logistic Regression is not very sensitive to scaling which is why the recall metric has the same score. \n",
    "\n",
    "        \n",
    "* The f-score does a very good job of summarising both recall and precision. \n",
    "    * **Uscaled LR**:0.70\n",
    "    * **Scaled LR**: 0.70\n",
    "    * We can infer from this Logistic Regression is not very sensitive to scaling which is why the f-score metric has the same score. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear SVC Observations for \"g_gas\" and \"g_ele\"\n",
    "\n",
    "As per the assignment we created a LinearSVC classifier. This classifier was applied to both target variables in an unscaled and scaled version. \n",
    "\n",
    "The scaling used was \"MinMax\" and it resulted in the following performance increases: \n",
    "\n",
    "* Our \"g_gas\" model had a performance increase of .33 after being scaled\n",
    "* Our \"g_ele\" model had a performance increase of .27 after being scaled\n",
    "\n",
    "We can conclude that preprocessing our data and making sure that it was scaled so that all features had values between 0 and 1 avoided certain features influencing the model more when compared to others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression Observations for \"g_gas\" and \"g_ele\"\n",
    "\n",
    "From the best score results corresponding to \"g_gas\" and \"g_ele\" we can see how there is no effect of scaling on the Logistic Regression models. From this we can conclude that scaling our data had no impact on the outcome and scores of the Logistic Regression models. \n",
    "\n",
    "The \"g_gas\" model when regularized works best when C is set to 0.001. On the other hand the \"g_ele\" model works best when C is set to 100. \n",
    "\n",
    "The \"g_ele\" model corresponds to a model that stresses the importance of correctly classifying every point. Even though it results in a relatively more complex model it is able to extrapolate succesfully based on the very similar results both for the training and the testing. This possibly suggests undefitting. Even less regularization (higher values for C) could be explored.\n",
    "\n",
    "From this we can conclude that regularization has improved generalization performance for both logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Assignment 2-B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_gas\" Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df, df_target['g_gas']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_gas\" Unscaled Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_unscaled = make_pipeline(SimpleImputer(strategy='mean'), DecisionTreeClassifier(random_state = 42))\n",
    "decision_tree_unscaled.fit(X_train, y_train)\n",
    "y_pred = decision_tree_unscaled.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(decision_tree_unscaled.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(decision_tree_unscaled.score(X_test, y_test)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_gas\" Scaled Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_scaled = make_pipeline(SimpleImputer(strategy='mean'), MinMaxScaler(),DecisionTreeClassifier(random_state = 42))\n",
    "decision_tree_scaled.fit(X_train, y_train)\n",
    "y_pred = decision_tree_scaled.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(decision_tree_scaled.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(decision_tree_scaled.score(X_test, y_test)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision Trees are invariant to scaling of the data. \n",
    "* However, Decision Trees are can become very complex models and end up having pure leaves. As a result of this complexity they are very prone to overfitting and not being able to generalize on the testing set. However this can be prevented by using either pre-pruning or pruning strategies, another strategy is to make us of ensembles of Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The confusion matrix is summarised using *precision* and *recall*.\n",
    "\n",
    "\n",
    "* Our value for precision which measures how many of the samples predicted as positive are actually positive is:\n",
    "    * **Unscaled DT**: 0.62\n",
    "    * **Scaled DT**: 0.61\n",
    "    * We can infer from this Decision Tree is not sensitive to scaling which is why the precision metric has aproximatley the same score. \n",
    "   \n",
    "   \n",
    "* Our value for recall measures just how many of our samples were labeled correctly. A higher recall value shows that the model does not produce many false positives.\n",
    "    * **Unscaled DT**: 0.62\n",
    "    * **Scaled DT**: 0.61\n",
    "    * We can infer from this Decision Tree is not sensitive to scaling which is why the recall metric has aproximatley the same score. \n",
    "\n",
    "\n",
    "        \n",
    "* The f-score does a very good job of summarising both recall and precision. \n",
    "    * **Uscaled DT**:0.62\n",
    "    * **Scaled DT**: 0.61\n",
    "    * We can infer from this Decision Tree is not sensitive to scaling which is why the F1 metrics have aproximatley the same score. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search on \"g_gas\" model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = make_pipeline(SimpleImputer(strategy='mean'), MinMaxScaler(),DecisionTreeClassifier(random_state = 42))\n",
    "param_grid = {'decisiontreeclassifier__max_depth': range(5,15,2), \n",
    "             'decisiontreeclassifier__max_features':range(40,59,5),\n",
    "             'decisiontreeclassifier__min_samples_leaf':range(1,10,1),\n",
    "             'decisiontreeclassifier__min_samples_split':[2,10,100]}\n",
    "grid = GridSearchCV(tree, param_grid, cv=10, return_train_score = True, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "y_pred = grid.predict(X_test)\n",
    "print(grid.best_params_)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Training accuracy:{}'.format(grid.score(X_train, y_train)))\n",
    "print('Test accuracy:{}'.format(grid.score(X_test, y_test)))\n",
    "\n",
    "model_name = 'decision tree'\n",
    "gas_fpr[model_name], gas_tpr[model_name], gas_roc_auc[model_name] = binarize_getroc(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_ele\" Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df, df_target['g_ele']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_ele\" Unscaled Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_unscaled = make_pipeline(SimpleImputer(strategy='mean'), DecisionTreeClassifier(random_state = 42))\n",
    "decision_tree_unscaled.fit(X_train, y_train)\n",
    "y_pred = decision_tree_unscaled.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(decision_tree_unscaled.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(decision_tree_unscaled.score(X_test, y_test)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_ele\" Scaled Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_scaled = make_pipeline(SimpleImputer(strategy='mean'), MinMaxScaler(),DecisionTreeClassifier(random_state = 42))\n",
    "decision_tree_scaled.fit(X_train, y_train)\n",
    "y_pred = decision_tree_scaled.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(decision_tree_scaled.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(decision_tree_scaled.score(X_test, y_test)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The confusion matrix is summarised using *precision* and *recall*.\n",
    "\n",
    "\n",
    "* Our value for precision which measures how many of the samples predicted as positive are actually positive is:\n",
    "    * **Unscaled DT**: 0.61\n",
    "    * **Scaled DT**: 0.61\n",
    "    * We can infer from this Decision Tree is not sensitive to scaling which is why the precision metric has the same score. \n",
    "   \n",
    "   \n",
    "* Our value for recall measures just how many of our samples were labeled correctly. A higher recall value shows that the model does not produce many false positives.\n",
    "    * **Unscaled DT**: 0.61\n",
    "    * **Scaled DT**: 0.61\n",
    "    * We can infer from this Decision Tree is not sensitive to scaling which is why the recall metric has the same score. \n",
    "\n",
    "\n",
    "        \n",
    "* The f-score does a very good job of summarising both recall and precision. \n",
    "    * **Uscaled DT**:0.61\n",
    "    * **Scaled DT**: 0.61\n",
    "    * We can infer from this Decision Tree is not sensitive to scaling which is why the F1 metrics has the same score. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search on \"g_ele\" model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = make_pipeline(SimpleImputer(strategy='mean'), MinMaxScaler(),DecisionTreeClassifier(random_state = 42))\n",
    "param_grid = {'decisiontreeclassifier__max_depth': range(5,15,2), \n",
    "             'decisiontreeclassifier__max_features':range(40,59,5),\n",
    "             'decisiontreeclassifier__min_samples_leaf':range(1,10,1),\n",
    "             'decisiontreeclassifier__min_samples_split':[2,10,100]}\n",
    "grid = GridSearchCV(tree, param_grid, cv=5, return_train_score = True, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "y_pred = decision_tree_scaled.predict(X_test)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Best score is {}\".format(grid.best_score_))\n",
    "print('Training accuracy:{}'.format(grid.score(X_train, y_train)))\n",
    "print('Test accuracy:{}'.format(grid.score(X_test, y_test)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)\n",
    "\n",
    "model_name = 'decision tree'\n",
    "ele_fpr[model_name], ele_tpr[model_name], ele_roc_auc[model_name] = binarize_getroc(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_gas\" Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df, df_target['g_gas']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_gas\" Unscaled Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_unscaled = make_pipeline(SimpleImputer(strategy='mean'), RandomForestClassifier(n_estimators = 100, random_state = 42))\n",
    "random_forest_unscaled.fit(X_train, y_train)\n",
    "random_forest_unscaled.score(X_test, y_test)\n",
    "y_pred = random_forest_unscaled.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(random_forest_unscaled.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(random_forest_unscaled.score(X_test, y_test)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_gas\" Scaled Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_scaled = make_pipeline(SimpleImputer(strategy='mean'),RandomForestClassifier(n_estimators = 100, random_state = 42))\n",
    "random_forest_scaled.fit(X_train, y_train)\n",
    "random_forest_scaled.score(X_test, y_test)\n",
    "y_pred = random_forest_scaled.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(random_forest_scaled.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(random_forest_scaled.score(X_test, y_test)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The confusion matrix is summarised using *precision* and *recall*.\n",
    "\n",
    "* Our value for precision which measures how many of the samples predicted as positive are actually positive is:\n",
    "    * **Unscaled RF**: 0.71\n",
    "    * **Scaled RF**: 0.71\n",
    "    * We can infer from this Random Forest is not sensitive to scaling which is why the precision metric has the same score. \n",
    "     \n",
    "     \n",
    "* Our value for recall measures just how many of our samples were labeled correctly. A higher recall value shows that the model does not produce many false positives.\n",
    "    * **Unscaled RF**: 0.72\n",
    "    * **Scaled RF**: 0.72\n",
    "    * We can infer from this Random Forest is not sensitive to scaling which is why the recall metric has the same score. \n",
    "\n",
    "        \n",
    "* The f-score does a very good job of summarising both recall and precision. \n",
    "    * **Uscaled RF**:0.71\n",
    "    * **Scaled RF**: 0.71\n",
    "    * We can infer from this Random Forest is not sensitive to scaling which is why the F1 metric has the same score. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search on \"g_gas\" model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = make_pipeline(SimpleImputer(strategy='mean'), MinMaxScaler(),RandomForestClassifier(random_state = 42))\n",
    "param_grid = {'randomforestclassifier__n_estimators': [10,100,200],\n",
    "              'randomforestclassifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'randomforestclassifier__max_depth' : [25,50,75],\n",
    "              'randomforestclassifier__min_samples_leaf' : [1,3,5],\n",
    "              'randomforestclassifier__min_samples_split' : [2,5,10]}\n",
    "grid = GridSearchCV(rfc, param_grid, cv=5, return_train_score = True, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_params_\n",
    "\n",
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(grid.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(grid.score(X_test, y_test)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)\n",
    "\n",
    "model_name = 'random forest'\n",
    "gas_fpr[model_name], gas_tpr[model_name], gas_roc_auc[model_name] = binarize_getroc(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_ele\" Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df, df_target['g_ele']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_ele\" Unscaled Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_unscaled = make_pipeline(SimpleImputer(strategy='mean'), RandomForestClassifier(n_estimators = 100, random_state = 42))\n",
    "random_forest_unscaled.fit(X_train, y_train)\n",
    "random_forest_unscaled.score(X_test, y_test)\n",
    "y_pred = random_forest_unscaled.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(random_forest_unscaled.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(random_forest_unscaled.score(X_test, y_test)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"g_ele\" Scaled Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_scaled = make_pipeline(SimpleImputer(strategy='mean'),MinMaxScaler(),RandomForestClassifier(n_estimators = 100, random_state = 42))\n",
    "random_forest_scaled.fit(X_train, y_train)\n",
    "random_forest_scaled.score(X_test, y_test)\n",
    "y_pred = random_forest_unscaled.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(random_forest_scaled.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(random_forest_scaled.score(X_test, y_test)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The confusion matrix is summarised using *precision* and *recall*.\n",
    "\n",
    "* Our value for precision which measures how many of the samples predicted as positive are actually positive is:\n",
    "    * **Unscaled RF**: 0.73\n",
    "    * **Scaled RF**: 0.73\n",
    "    * We can infer from this Random Forest is not sensitive to scaling which is why the precision metric has the same score. \n",
    "     \n",
    "     \n",
    "* Our value for recall measures just how many of our samples were labeled correctly. A higher recall value shows that the model does not produce many false positives.\n",
    "    * **Unscaled RF**: 0.73\n",
    "    * **Scaled RF**: 0.73\n",
    "    * We can infer from this Random Forest is not sensitive to scaling which is why the recall metric has the same score. \n",
    "\n",
    "        \n",
    "* The f-score does a very good job of summarising both recall and precision. \n",
    "    * **Uscaled RF**:0.73\n",
    "    * **Scaled RF**: 0.73\n",
    "    * We can infer from this Random Forest is not sensitive to scaling which is why the F1 metric has the same score. \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random Forests are also completely invariant to scaling of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How are decision tree classifiers different from random forests on a structural level? \n",
    "##### Discuss their advantages/disadvantages over each other and where would you choose one over the other?\n",
    "\n",
    "* Random Forests are different from Decision Trees on a strcutural level because Random Forests are an ensemble of unique and random Decision Trees. This allows us to reduce the level of overfitting by creating an average of the aggregated individual Decision Trees that make up the forest. Random Forests are also randomly created by bootstrapping the data which creates a significant structural difference when compared to Decision Trees. \n",
    "\n",
    "\n",
    "* However, given that Random Forests are created using multiple Decision Trees this produces models that are very complex and not suited for compact visual representations; this simpler visualization can be achieved with a Decision Tree model on the other hand. Both models work without scaling the data and do not require heavy tuning of parameters.\n",
    "\n",
    "\n",
    "* For simple to understand models Decision Trees are more suitable, however if this is not an issue we would use a Random Forest. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search on \"g_ele\" model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = make_pipeline(SimpleImputer(strategy='mean'),MinMaxScaler(),RandomForestClassifier(random_state = 42))\n",
    "param_grid = {'randomforestclassifier__n_estimators': [10,100,200],\n",
    "              'randomforestclassifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'randomforestclassifier__max_depth' : [25,50,75],\n",
    "              'randomforestclassifier__min_samples_leaf' : [1,3,5],\n",
    "              'randomforestclassifier__min_samples_split' : [2,5,10]}\n",
    "grid = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_params_\n",
    "\n",
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(grid.score(X_train, y_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(grid.score(X_test, y_test)))\n",
    "auc_score = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"The AUC score for this model is:\", auc_score)\n",
    "\n",
    "model_name = 'random forest'\n",
    "ele_fpr[model_name], ele_tpr[model_name], ele_roc_auc[model_name] = binarize_getroc(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves\n",
    "#### ROC Curves for \"g_gas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all ROC curves for gas\n",
    "gas = plt.figure()\n",
    "gaseax = gas.add_subplot(1, 1, 1)\n",
    "\n",
    "for model_name in gas_fpr:\n",
    "    gaseax.plot(gas_fpr[model_name][\"marco\"], gas_tpr[model_name][\"macro\"], label='macro-average ROC curve for {:s} (area = {:0.2f})'.format(model_name, gas_roc_auc[model_name][\"macro\"]))\n",
    "\n",
    "gaseax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "gaseax.set_xlim([0.0, 1.0])\n",
    "gaseax.set_ylim([0.0, 1.05])\n",
    "gaseax.set_xlabel('False Positive Rate')\n",
    "gaseax.set_ylabel('True Positive Rate')\n",
    "gaseax.set_title('Receiver operating characteristic - gas')\n",
    "gas.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curves for \"g_ele\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all ROC curves for electricity\n",
    "ele = plt.figure()\n",
    "eleeax = ele.add_subplot(1, 1, 1)\n",
    "\n",
    "for model_name in ele_fpr:\n",
    "    eleeax.plot(ele_fpr[model_name][\"macro\"], ele_tpr[model_name][\"macro\"],label='macro-average ROC curve for {:s} (area = {:0.2f})'.format(model_name, ele_roc_auc[model_name][\"macro\"]))\n",
    "\n",
    "eleeax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "eleeax.set_xlim([0.0, 1.0])\n",
    "eleeax.set_ylim([0.0, 1.05])\n",
    "eleeax.set_xlabel('False Positive Rate')\n",
    "eleeax.set_ylabel('True Positive Rate')\n",
    "eleeax.set_title('Receiver operating characteristic - electricity')\n",
    "ele.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is accuracy well suited to these classifications task?\n",
    "\n",
    "* Accuracy is not well suited because the samples are not balanced, therefore ROC-AUC is preferable. \n",
    "\n",
    "#### Can we find the optimal model?\n",
    "\n",
    "* Yes, we can select the optimal model. \n",
    "    * The optimal model for this classification task is the Random Forest. This can be seen from both ROC graphs and their respective AUC scores, that were printed for each model. Logisitc Regression and Linear SVC can also be considered a good alternative to the Random Forest model, and require significantly less computing power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
